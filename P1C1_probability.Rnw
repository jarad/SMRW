\chapter{Probability}

In this chapter, 
we will be introducing the mathematical concept of probability. 
But before we do so, 
it makes sense to consider how we use the term probability in our life and 
what we really mean when we say \emph{probability}.

\section{Interpretation of Probability}

I googled for news articles using the word probability and here are some
titles of articles:

\begin{quote}
A recent paper in Reviews of Geophysics describes a probabilistic method for evaluating tsunami location, size, and risk to human populations.
\end{quote}

from \url{https://eos.org/editors-vox/probability-analysis-improves-hazard-assessment}

Without reading the paper, 
I believe that the paper is trying to predict where and when a tsunami will
hit as well as the expected strength. 

\begin{quote}
Chiefsâ€™ win probability was nearly 98 percent at one point against Titans.
\end{quote}

from \url{http://www.kansascity.com/sports/spt-columns-blogs/for-petes-sake/article193395329.html}

As you might expect the Chiefs' end up losing the game. 

\begin{quote}
Recession Probability Ticks Slightly Up As Fed Hikes
\end{quote}

from \url{https://seekingalpha.com/article/4132021-recession-probability-ticks-slightly-fed-hikes}

Apparently the probability of a recession occurring within the next 12 months 
has ticked up from 0.37\% to 0.40\%. 

\begin{quote}
Column: The probability of being right or wrong
\end{quote}

from \url{http://www.tampabay.com/opinion/columns/Column-The-probability-of-being-right-or-wrong_163910545}.

This article was accompanied with two 6-sided die that we both on 2. 
I'm not sure if that was significant. 

\subsection{Frequency}

In most introductory probability and statistics textbooks, 
the common interpretation of probability is that it is a relative frequency
of observing that event amongst all the times that the event could have 
occurred.
More formally, it is the limit of all these times and we can express this 
mathematically in \eqref{e:frequency}.
\begin{equation}
\label{e:frequency}
P(E) = \lim_{n\to\infty} \frac{\sum_{i=1}^n X_i}{n}.
\end{equation}

Equation \eqref{e:frequency} looks complicated, 
but it isn't really too bad. 
Let's break down the parts.
On the left-hand side, 
we have 
\begin{itemize}
\item $P(E)$ which is our notation for the probability of the event $E$. 
\end{itemize}
In our examples,
these probabilities may be for the following events:
\begin{enumerate}
\item a tsunami of intensity at least 1\footnote{Suppose I am using the Imamuri-lida tsunami intensity scale.} hitting Biesanz Beach in Costa
Rica within the next year,
\item Chiefs winning the game against the Titans,
\item A U.S. recession occurring in the next 12 months, or
\item being wrong about something, say thinking the Chiefs were going to win.
\end{enumerate}

The left-hand side is our notation and we are defining probability by the terms
on the right. 

\begin{itemize}
\item $X_i$ is an indicator of event $E$ occurring in the $i$th trial, 
i.e. $X_i$ is 1 if the trial occurred in the $i$th attempt and 0 otherwise.
\item $\sum_{i=1}^n X_i$ is the sum of these indicators and is therefore the
number of times the event has occurred in the first $n$ attempts.
\item $\frac{\sum_{i=1}^n X_i}{n}$ is the number of times the event has occurred
divided by the number of attempts and is thus the proportion of times the 
event has occurred in the $n$ attempts.
\item $\lim_{n\to\infty}$ just means to keep making attempts so that $n$ gets 
larger and larger. 
\end{itemize}

So as we keep making attempts, 
presumably the proportion settles down to one particular number and we call that
number the \emph{probability}.

This interpretation makes complete sense in theory.
So, suppose we roll a 6-sided die many times and record how often it 
lands face-side up on a 3. 
Instead of actually performing this experiment in my office, 
I am going to use a random number generator in R to do it. 
So, here is the experiment:

<<roll_die>>=
set.seed(20180608)
n <- 10000
x <- sample(6,                 # a 6-sided die
            size = n,    # number of attempts
            replace = TRUE,    # we are allowed to roll the same number again
            prob = rep(1/6,6)) # probability is 1/6 for every side
@

Wasn't that exciting!
Well, maybe a picture will help.

<<plot_die_rolls, dependson = "roll_die">>=
p <- cumsum(x == 3)/(1:n)
plot(p, 
     xlab = 'Number of attempts', 
     ylab = 'Proportion', 
     main = '', type = 'l')
abline(h=1/6, col='red')
@

Indeed, 
this relative frequency interpretation makes a lot of sense when we consider 
games of chance, 
but it appears to have less applicability to many real-world situations like
the ones discussed above.
Can we ever repeatedly attempt to get a tsunami at a particular place at a 
particular time? 
Can we ever repeatedly play that game pitting the Chiefs against the Titans?
Can we ever repeatedly have January to December of 2018?



\subsection{Personal belief}

An alternative to a relative frequency interpretation is an interpretation 
based on personal belief. 
That is, probability is a statement about your belief that an event will occur.
So, we can have our personal belief about 
\begin{enumerate}
\item a tsunami of intensity at least 1 hitting Biesanz Beach in Costa
Rica within the next year,
\item Chiefs winning the game against the Titans,
\item A U.S. recession occurring in the next 12 months, or
\item being wrong about something, say thinking the Chiefs were going to win.
\end{enumerate}

There is no reason we need to agree on this belief as we have different 
background knowledge.
That is, 
if you have background knowledge $K_1$ and I have background $K_2$, 
then we can have 
\[ P(E|K_1) \ne P(E|K_2) \]
where the notation is a conditional probability, 
e.g. $P(E|K_2)$ is my personal belief about event $E$ given that my background
knowledge is $K_2$.
But if we collect enough data $y$, 
then we might be close, i.e. 
\[ P(E|K_1,y) \approx P(E|K_2,y). \]
If these probabilities are close enough, 
then we will make the same decisions regarding the event $E$. 

This approach also works for games of chance.
Typically in games of chance, 
we can deductively arrive at a probability that we
will all agree on, 
e.g. most people agree that the probability of rolling a 3 on a 6-sided die 
is 1/6 (or at least extremely close to this value).


\section{Probability theory}

While keeping our thoughts on the interpretation of probability, 
we will now turn to probability theory as defined using mathematics.
The language of probability is initial based on set theory, 
then postulates a set of axioms, and adds a couple of definitions.


\subsection{Set theory}

Throughout the following, 
we will denote a set using a capitalize Roman letter and the elements of a 
set within curly braces, i.e. $\{ \}$. 

\begin{definition}
A \emph{set} is a collection of things and those things are called 
\emph{elements} of the set.
\end{definition}

For example, 
\begin{itemize}
\item the possible rolls of a standard 6-sided die, i.e. $A = \{1,2,3,4,5,6\}$,
\item even, positive integers, i.e. $B = \{2,4,6,\ldots\}$, and
\item all numbers in the interval 0 to 3, i.e. $C = (0,3)$ 
(this is the open interval that does not include 0 or 3).
\end{itemize}
These examples all use numbers, but a set can also be actual objects, e.g. 
$D = \{\mbox{apple}, \mbox{orange}, \mbox{banana}\}$.


\begin{definition}
A \emph{subset} of a set, denoted $\subseteq$, occurs when another set has some 
(possibly all) but no extra elements of a set.
A \emph{proper subset}, denoted $\subset$ is a subset that does not have 
all the elements of another set.
\end{definition}

For example, 
\[
\{1,2,3\} \subset A
\]
and 
\[
\{2,4,6\} \subset A.
\]



There some standard mathematical operations that can be performed on sets. 

\begin{definition}
The \emph{union}, denoted $\union$, of two sets includes any element in either 
set.
\end{definition}

For example,
\[ 
A \union B = \{1,2,3,4,5,6,8,10,12,\ldots\}.
\]

Notice that the elements are not repeated, 
there is only a single 2, 4, and 6 in this union.

\begin{definition}
The \emph{intersection}, denoted $\intersection$, 
of two sets includes any element in both sets.
\end{definition}

For example,
\[ 
A \intersection B = \{2,4,6\}.
\]


\begin{definition}
The \emph{set difference}, denoted $\setminus$, 
of two sets is the elements in the first that do not occur in the second.
\end{definition}

For example,
\[
A \setminus B = \{1,3,5\}
\]
and 
\[
B \setminus A = \{8,10,12,\ldots\}.
\]






When used for the purpose of probability, 
we often consider constructing a special set of all possible outcomes of an 
experiment.
We call this set the \emph{sample space} and typically denote it $\Omega$.
Thus the sample space for a single roll of a 6-sided die is $\{1,2,3,4,5,6\}$.

Subsets of this sample space are called events

